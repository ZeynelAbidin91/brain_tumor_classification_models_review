{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "insured-august",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "unnecessary-boost",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import itertools\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly import tools\n",
    "\n",
    "from keras import layers, models\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dense, Flatten, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.applications import VGG19\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications import ResNet101\n",
    "from keras.applications import DenseNet201, DenseNet169, DenseNet121\n",
    "from keras.applications import InceptionV3\n",
    "from keras.applications import InceptionResNetV2\n",
    "from keras.applications import Xception\n",
    "from keras.applications import MobileNetV2\n",
    "from keras.applications import MobileNet\n",
    "from keras import layers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "RANDOM_SEED = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alternate-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dir_path, img_size=(100, 100)):\n",
    "    X = []\n",
    "    y = []\n",
    "    i = 0\n",
    "    labels = dict()\n",
    "    for path in tqdm(sorted(os.listdir(dir_path))):\n",
    "        if not path.startswith('.'):\n",
    "            labels[i] = path\n",
    "            for file in os.listdir(dir_path + path):\n",
    "                if not file.startswith('.'):\n",
    "                    img = cv2.imread(dir_path + path + '/' + file)\n",
    "                    X.append(img)\n",
    "                    y.append(i)\n",
    "            i += 1\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    print('{} images loaded from {} directory.'.format(len(X), dir_path))\n",
    "    return X, y, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "declared-chile",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.44it/s]\n",
      "C:\\Users\\zeyne\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:15: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.88it/s]\n",
      " 50%|██████████████████████████████████████████                                          | 1/2 [00:00<00:00,  7.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242 images loaded from TRAIN/ directory.\n",
      "19 images loaded from TEST/ directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 images loaded from VAL/ directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DIR = 'TRAIN/'\n",
    "TEST_DIR = 'TEST/'\n",
    "VAL_DIR = 'VAL/'\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "X_train, y_train, labels = load_data(TRAIN_DIR, IMG_SIZE)\n",
    "X_test, y_test, _ = load_data(TEST_DIR, IMG_SIZE)\n",
    "X_val, y_val, _ = load_data(VAL_DIR, IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "irish-worth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_imgs(set_name, img_size):\n",
    "    \n",
    "    # Resize and apply VGG-15 preprocessing\n",
    "    set_new = []\n",
    "    for img in set_name:\n",
    "        img = cv2.resize(\n",
    "            img,\n",
    "            dsize=img_size,\n",
    "            interpolation=cv2.INTER_CUBIC\n",
    "        )\n",
    "        set_new.append(preprocess_input(img))\n",
    "    return np.array(set_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "closed-gentleman",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prep = preprocess_imgs(set_name=X_train, img_size=IMG_SIZE)\n",
    "X_test_prep = preprocess_imgs(set_name=X_test, img_size=IMG_SIZE)\n",
    "X_val_prep = preprocess_imgs(set_name=X_val, img_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-jackson",
   "metadata": {},
   "source": [
    "# CNN Pre-trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-district",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "backed-purpose",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 242 images belonging to 2 classes.\n",
      "Found 92 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    brightness_range=[0.5, 1.5],\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    color_mode='rgb',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=8,\n",
    "    class_mode='binary',\n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    color_mode='rgb',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=8,\n",
    "    class_mode='binary',\n",
    "    seed=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "corresponding-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading base model\n",
    "def load_feature_extractor():\n",
    "    base_model = ResNet101(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=IMG_SIZE + (3,))\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "desperate-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_feature_extractor()\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pleasant-peace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(img_data):\n",
    "    \n",
    "    features = []\n",
    "    for img in img_data:\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        feature = model.predict(img)\n",
    "        feature_np = np.array(feature)\n",
    "        features.append(feature_np.flatten())\n",
    "    \n",
    "    feature_list_np = np.array(features)\n",
    "    \n",
    "    return feature_list_np\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "united-forestry",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ftr = extract_features(X_train_prep)\n",
    "y_train_ftr = y_train\n",
    "X_test_ftr = extract_features(X_test_prep)\n",
    "y_test_ftr = y_test\n",
    "X_val_ftr = extract_features(X_val_prep)\n",
    "y_val_ftr = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hazardous-portable",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 100352)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_ftr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-trunk",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cheap-evidence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(random_state=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier_SVM = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier_SVM.fit(X_train_ftr, y_train_ftr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-narrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_SVM = classifier_SVM.score(X_train_ftr, y_train_ftr)\n",
    "val_acc_SVM = classifier_SVM.score(X_val_ftr, y_val_ftr)\n",
    "test_acc_SVM = classifier_SVM.score(X_test_ftr, y_test_ftr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-bargain",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(train_acc_SVM, 4))\n",
    "print(np.round(val_acc_SVM, 4))\n",
    "print(np.round(test_acc_SVM, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-breakfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_SVM = classifier_SVM.predict(X_test_ftr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def print_performance_metrics(test_labels,predict):\n",
    "    print('F1 Score:', np.round(metrics.f1_score(test_labels, predict,\n",
    "                                               average='weighted'),4))\n",
    "    print('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(test_labels, predict),4))\n",
    "    print('Recall:', np.round(metrics.recall_score(test_labels, predict,\n",
    "                                               average='weighted'),4))\n",
    "    print('Precision:', np.round(metrics.precision_score(test_labels, predict,average='weighted'),4))\n",
    "    print('Accuracy:', np.round(metrics.accuracy_score(test_labels, predict),4))\n",
    "    print('ROC Area:', np.round(metrics.roc_auc_score(test_labels, predict),4))\n",
    "    print('Recall:', np.round(metrics.recall_score(test_labels, predict,\n",
    "                                               average='weighted'),4))\n",
    "    print('Matthews Corrcoef:', np.round(metrics.matthews_corrcoef(test_labels, predict),4)) \n",
    "    print('\\t\\tClassification Report:\\n', metrics.classification_report(test_labels, predict))\n",
    "\n",
    "print_performance_metrics(y_test,y_pred_SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-pattern",
   "metadata": {},
   "source": [
    "### RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-peripheral",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier_RF = RandomForestClassifier(n_estimators = 800, criterion = 'entropy',  random_state = 0)\n",
    "classifier_RF.fit(X_train_ftr, y_train_ftr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-register",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_RF = classifier_RF.score(X_train_ftr, y_train_ftr)\n",
    "print(np.round(train_acc_RF, 4))\n",
    "val_acc_RF = classifier_RF.score(X_val_ftr, y_val_ftr)\n",
    "print(np.round(val_acc_RF, 4))\n",
    "test_acc_RF = classifier_RF.score(X_test_ftr, y_test_ftr)\n",
    "print(np.round(test_acc_RF, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-intermediate",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_RF = classifier_RF.predict(X_test_ftr)\n",
    "print_performance_metrics(y_test,y_pred_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-validity",
   "metadata": {},
   "source": [
    "### ADABOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-postage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "classifier_AB = AdaBoostClassifier(n_estimators = 100)\n",
    "classifier_AB.fit(X_train_ftr, y_train_ftr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_AB = classifier_AB.score(X_train_ftr, y_train_ftr)\n",
    "print(train_acc_AB)\n",
    "val_acc_AB = classifier_AB.score(X_val_ftr, y_val_ftr)\n",
    "print(val_acc_AB)\n",
    "test_acc_AB = classifier_AB.score(X_test_ftr, y_test_ftr)\n",
    "print(test_acc_AB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-warrior",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_AB = classifier_AB.predict(X_test_ftr)\n",
    "print_performance_metrics(y_test,y_pred_AB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-worth",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-phone",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier_kNN = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\n",
    "classifier_kNN.fit(X_train_ftr, y_train_ftr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-cooking",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_kNN = classifier_kNN.score(X_train_ftr, y_train_ftr)\n",
    "print(train_acc_kNN)\n",
    "val_acc_kNN = classifier_kNN.score(X_val_ftr, y_val_ftr)\n",
    "print(val_acc_kNN)\n",
    "test_acc_kNN = classifier_kNN.score(X_test_ftr, y_test_ftr)\n",
    "print(test_acc_kNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-titanium",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_kNN = classifier_kNN.predict(X_test_ftr)\n",
    "print_performance_metrics(y_test,y_pred_kNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-anthropology",
   "metadata": {},
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-quarterly",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "classifier_xgb = xgb.XGBClassifier(n_estimators = 300)\n",
    "classifier_xgb.fit(X_train_ftr, y_train_ftr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-greece",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_xgb = classifier_xgb.score(X_train_ftr, y_train_ftr)\n",
    "print(train_acc_xgb)\n",
    "val_acc_xgb = classifier_xgb.score(X_val_ftr, y_val_ftr)\n",
    "print(val_acc_xgb)\n",
    "test_acc_xgb = classifier_xgb.score(X_test_ftr, y_test_ftr)\n",
    "print(test_acc_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb = classifier_xgb.predict(X_test_ftr)\n",
    "print_performance_metrics(y_test,y_pred_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-companion",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_ANN():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape = (last_layer_shape[1], last_layer_shape[2], last_layer_shape[3])))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-campus",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ANN = model_ANN()\n",
    "\n",
    "\n",
    "model_ANN.summary()\n",
    "model_ANN.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=RMSprop(lr=1e-4),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model_ANN.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-probability",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_ANN = model_ANN.fit(\n",
    "    X_train_ftr, y_train_ftr, \n",
    "    epochs=40, \n",
    "    verbose=1, \n",
    "    validation_data=(X_val_ftr,y_val_ftr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-coordinate",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
