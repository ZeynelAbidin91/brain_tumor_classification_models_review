{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "insured-august",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "unnecessary-boost",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import itertools\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly import tools\n",
    "\n",
    "from keras import layers, models\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dense, Flatten, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.applications import VGG19\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications import ResNet101\n",
    "from keras.applications import DenseNet201, DenseNet169, DenseNet121\n",
    "from keras.applications import InceptionV3\n",
    "from keras.applications import InceptionResNetV2\n",
    "from keras.applications import Xception\n",
    "from keras.applications import MobileNetV2\n",
    "from keras.applications import MobileNet\n",
    "from keras import layers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "RANDOM_SEED = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alternate-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dir_path, img_size=(100, 100)):\n",
    "    X = []\n",
    "    y = []\n",
    "    i = 0\n",
    "    labels = dict()\n",
    "    for path in tqdm(sorted(os.listdir(dir_path))):\n",
    "        if not path.startswith('.'):\n",
    "            labels[i] = path\n",
    "            for file in os.listdir(dir_path + path):\n",
    "                if not file.startswith('.'):\n",
    "                    img = cv2.imread(dir_path + path + '/' + file)\n",
    "                    X.append(img)\n",
    "                    y.append(i)\n",
    "            i += 1\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    print('{} images loaded from {} directory.'.format(len(X), dir_path))\n",
    "    return X, y, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "declared-chile",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.27it/s]\n",
      "C:\\Users\\zeyne\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:15: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 37.02it/s]\n",
      " 50%|██████████████████████████████████████████                                          | 1/2 [00:00<00:00,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242 images loaded from TRAIN/ directory.\n",
      "19 images loaded from TEST/ directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 images loaded from VAL/ directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DIR = 'TRAIN/'\n",
    "TEST_DIR = 'TEST/'\n",
    "VAL_DIR = 'VAL/'\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "X_train, y_train, labels = load_data(TRAIN_DIR, IMG_SIZE)\n",
    "X_test, y_test, _ = load_data(TEST_DIR, IMG_SIZE)\n",
    "X_val, y_val, _ = load_data(VAL_DIR, IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "irish-worth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_imgs(set_name, img_size):\n",
    "    \n",
    "    # Resize and apply VGG-15 preprocessing\n",
    "    set_new = []\n",
    "    for img in set_name:\n",
    "        img = cv2.resize(\n",
    "            img,\n",
    "            dsize=img_size,\n",
    "            interpolation=cv2.INTER_CUBIC\n",
    "        )\n",
    "        set_new.append(preprocess_input(img))\n",
    "    return np.array(set_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "closed-gentleman",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prep = preprocess_imgs(set_name=X_train, img_size=IMG_SIZE)\n",
    "X_test_prep = preprocess_imgs(set_name=X_test, img_size=IMG_SIZE)\n",
    "X_val_prep = preprocess_imgs(set_name=X_val, img_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-jackson",
   "metadata": {},
   "source": [
    "# CNN Pre-trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-district",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "backed-purpose",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 242 images belonging to 2 classes.\n",
      "Found 92 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    brightness_range=[0.5, 1.5],\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    color_mode='rgb',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=8,\n",
    "    class_mode='binary',\n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    color_mode='rgb',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=8,\n",
    "    class_mode='binary',\n",
    "    seed=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "corresponding-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading base model\n",
    "def load_feature_extractor():\n",
    "    base_model = MobileNetV2(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=IMG_SIZE + (3,))\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "desperate-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_feature_extractor()\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pleasant-peace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(img_data):\n",
    "    \n",
    "    features = []\n",
    "    for img in img_data:\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        feature = model.predict(img)\n",
    "        feature_np = np.array(feature)\n",
    "        features.append(feature_np.flatten())\n",
    "    \n",
    "    feature_list_np = np.array(features)\n",
    "    \n",
    "    return feature_list_np\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "united-forestry",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ftr = extract_features(X_train_prep)\n",
    "y_train_ftr = y_train\n",
    "X_test_ftr = extract_features(X_test_prep)\n",
    "y_test_ftr = y_test\n",
    "X_val_ftr = extract_features(X_val_prep)\n",
    "y_val_ftr = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hazardous-portable",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 62720)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_ftr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-trunk",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cheap-evidence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(random_state=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier_SVM = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier_SVM.fit(X_train_ftr, y_train_ftr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "professional-narrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_SVM = classifier_SVM.score(X_train_ftr, y_train_ftr)\n",
    "val_acc_SVM = classifier_SVM.score(X_val_ftr, y_val_ftr)\n",
    "test_acc_SVM = classifier_SVM.score(X_test_ftr, y_test_ftr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "expensive-bargain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9752\n",
      "0.9674\n",
      "0.9474\n"
     ]
    }
   ],
   "source": [
    "print(np.round(train_acc_SVM, 4))\n",
    "print(np.round(val_acc_SVM, 4))\n",
    "print(np.round(test_acc_SVM, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "weekly-breakfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_SVM = classifier_SVM.predict(X_test_ftr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "stunning-blond",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9474\n",
      "Cohen Kappa Score: 0.895\n",
      "Recall: 0.9474\n",
      "Precision: 0.9526\n",
      "Accuracy: 0.9474\n",
      "ROC Area: 0.95\n",
      "Recall: 0.9474\n",
      "Matthews Corrcoef: 0.9\n",
      "\t\tClassification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95        10\n",
      "           1       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.95        19\n",
      "   macro avg       0.95      0.95      0.95        19\n",
      "weighted avg       0.95      0.95      0.95        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "def print_performance_metrics(test_labels,predict):\n",
    "    print('F1 Score:', np.round(metrics.f1_score(test_labels, predict,\n",
    "                                               average='weighted'),4))\n",
    "    print('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(test_labels, predict),4))\n",
    "    print('Recall:', np.round(metrics.recall_score(test_labels, predict,\n",
    "                                               average='weighted'),4))\n",
    "    print('Precision:', np.round(metrics.precision_score(test_labels, predict,average='weighted'),4))\n",
    "    print('Accuracy:', np.round(metrics.accuracy_score(test_labels, predict),4))\n",
    "    print('ROC Area:', np.round(metrics.roc_auc_score(test_labels, predict),4))\n",
    "    print('Recall:', np.round(metrics.recall_score(test_labels, predict,\n",
    "                                               average='weighted'),4))\n",
    "    print('Matthews Corrcoef:', np.round(metrics.matthews_corrcoef(test_labels, predict),4)) \n",
    "    print('\\t\\tClassification Report:\\n', metrics.classification_report(test_labels, predict))\n",
    "\n",
    "print_performance_metrics(y_test,y_pred_SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-pattern",
   "metadata": {},
   "source": [
    "### RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "unlikely-peripheral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', n_estimators=800, random_state=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier_RF = RandomForestClassifier(n_estimators = 800, criterion = 'entropy',  random_state = 0)\n",
    "classifier_RF.fit(X_train_ftr, y_train_ftr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "integral-register",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9783\n",
      "0.9474\n"
     ]
    }
   ],
   "source": [
    "train_acc_RF = classifier_RF.score(X_train_ftr, y_train_ftr)\n",
    "print(np.round(train_acc_RF, 4))\n",
    "val_acc_RF = classifier_RF.score(X_val_ftr, y_val_ftr)\n",
    "print(np.round(val_acc_RF, 4))\n",
    "test_acc_RF = classifier_RF.score(X_test_ftr, y_test_ftr)\n",
    "print(np.round(test_acc_RF, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "extensive-intermediate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9474\n",
      "Cohen Kappa Score: 0.895\n",
      "Recall: 0.9474\n",
      "Precision: 0.9526\n",
      "Accuracy: 0.9474\n",
      "ROC Area: 0.95\n",
      "Recall: 0.9474\n",
      "Matthews Corrcoef: 0.9\n",
      "\t\tClassification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95        10\n",
      "           1       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.95        19\n",
      "   macro avg       0.95      0.95      0.95        19\n",
      "weighted avg       0.95      0.95      0.95        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_RF = classifier_RF.predict(X_test_ftr)\n",
    "print_performance_metrics(y_test,y_pred_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-validity",
   "metadata": {},
   "source": [
    "### ADABOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "imperial-postage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(n_estimators=100)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "classifier_AB = AdaBoostClassifier(n_estimators = 100)\n",
    "classifier_AB.fit(X_train_ftr, y_train_ftr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "needed-sacrifice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "train_acc_AB = classifier_AB.score(X_train_ftr, y_train_ftr)\n",
    "print(train_acc_AB)\n",
    "val_acc_AB = classifier_AB.score(X_val_ftr, y_val_ftr)\n",
    "print(val_acc_AB)\n",
    "test_acc_AB = classifier_AB.score(X_test_ftr, y_test_ftr)\n",
    "print(test_acc_AB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "extended-warrior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 1.0\n",
      "Cohen Kappa Score: 1.0\n",
      "Recall: 1.0\n",
      "Precision: 1.0\n",
      "Accuracy: 1.0\n",
      "ROC Area: 1.0\n",
      "Recall: 1.0\n",
      "Matthews Corrcoef: 1.0\n",
      "\t\tClassification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           1.00        19\n",
      "   macro avg       1.00      1.00      1.00        19\n",
      "weighted avg       1.00      1.00      1.00        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_AB = classifier_AB.predict(X_test_ftr)\n",
    "print_performance_metrics(y_test,y_pred_AB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-worth",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "amazing-phone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='ball_tree')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier_kNN = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\n",
    "classifier_kNN.fit(X_train_ftr, y_train_ftr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "spiritual-cooking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8471074380165289\n",
      "0.8478260869565217\n",
      "0.8421052631578947\n"
     ]
    }
   ],
   "source": [
    "train_acc_kNN = classifier_kNN.score(X_train_ftr, y_train_ftr)\n",
    "print(train_acc_kNN)\n",
    "val_acc_kNN = classifier_kNN.score(X_val_ftr, y_val_ftr)\n",
    "print(val_acc_kNN)\n",
    "test_acc_kNN = classifier_kNN.score(X_test_ftr, y_test_ftr)\n",
    "print(test_acc_kNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "constant-titanium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8412\n",
      "Cohen Kappa Score: 0.6816\n",
      "Recall: 0.8421\n",
      "Precision: 0.8451\n",
      "Accuracy: 0.8421\n",
      "ROC Area: 0.8389\n",
      "Recall: 0.8421\n",
      "Matthews Corrcoef: 0.6854\n",
      "\t\tClassification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86        10\n",
      "           1       0.88      0.78      0.82         9\n",
      "\n",
      "    accuracy                           0.84        19\n",
      "   macro avg       0.85      0.84      0.84        19\n",
      "weighted avg       0.85      0.84      0.84        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_kNN = classifier_kNN.predict(X_test_ftr)\n",
    "print_performance_metrics(y_test,y_pred_kNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-anthropology",
   "metadata": {},
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "particular-quarterly",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zeyne\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:05:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=300, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "classifier_xgb = xgb.XGBClassifier(n_estimators = 300)\n",
    "classifier_xgb.fit(X_train_ftr, y_train_ftr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "still-greece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "train_acc_xgb = classifier_xgb.score(X_train_ftr, y_train_ftr)\n",
    "print(train_acc_xgb)\n",
    "val_acc_xgb = classifier_xgb.score(X_val_ftr, y_val_ftr)\n",
    "print(val_acc_xgb)\n",
    "test_acc_xgb = classifier_xgb.score(X_test_ftr, y_test_ftr)\n",
    "print(test_acc_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "worldwide-evaluation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 1.0\n",
      "Cohen Kappa Score: 1.0\n",
      "Recall: 1.0\n",
      "Precision: 1.0\n",
      "Accuracy: 1.0\n",
      "ROC Area: 1.0\n",
      "Recall: 1.0\n",
      "Matthews Corrcoef: 1.0\n",
      "\t\tClassification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           1.00        19\n",
      "   macro avg       1.00      1.00      1.00        19\n",
      "weighted avg       1.00      1.00      1.00        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_xgb = classifier_xgb.predict(X_test_ftr)\n",
    "print_performance_metrics(y_test,y_pred_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-companion",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "complimentary-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_ANN():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape = (last_layer_shape[1], last_layer_shape[2], last_layer_shape[3])))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adjusted-campus",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-fd4df2b8f4fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_ANN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_ANN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel_ANN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m model_ANN.compile(\n",
      "\u001b[1;32m<ipython-input-30-a8d51be5ca2a>\u001b[0m in \u001b[0;36mmodel_ANN\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmodel_ANN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlast_layer_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_layer_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_layer_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Input' is not defined"
     ]
    }
   ],
   "source": [
    "model_ANN = model_ANN()\n",
    "\n",
    "\n",
    "model_ANN.summary()\n",
    "model_ANN.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=RMSprop(lr=1e-4),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model_ANN.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-probability",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_ANN = model_ANN.fit(\n",
    "    X_train_ftr, y_train_ftr, \n",
    "    epochs=40, \n",
    "    verbose=1, \n",
    "    validation_data=(X_val_ftr,y_val_ftr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-coordinate",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
